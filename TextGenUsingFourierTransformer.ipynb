{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a1e4350d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "import os\n",
    "import re\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b2768068",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Defining hyperparameters\n",
    "\n",
    "VOCAB_SIZE = 8192\n",
    "MAX_SAMPLES = 50000\n",
    "BUFFER_SIZE = 20000\n",
    "MAX_LENGTH = 40\n",
    "EMBED_DIM = 256\n",
    "LATENT_DIM = 512\n",
    "NUM_HEADS = 8\n",
    "BATCH_SIZE = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "29561070",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"rsp_story_data.csv\")\n",
    "df_trimmed = df.drop(columns=['ExternalId','ReleaseUId','Unnamed: 0'])\n",
    "df_trimmed = df_trimmed.dropna(axis=0)\n",
    "df_trimmed['StoryTitleDescription'] = df_trimmed['Title'] + \" \" + df_trimmed['Description']\n",
    "df_trimmed['Acc'] = df_trimmed['AcceptanceCriteria']\n",
    "df_trimmed = df_trimmed.drop(columns=['Title','Description'],axis=1)\n",
    "df_trimmed = df_trimmed.drop(columns=['AcceptanceCriteria'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "feadbd6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_punctuation(input):\n",
    "    #puctuation_pattern = re.compile(r'[%s]'%string.punctuation\n",
    "    puctuation_pattern=re.compile(r'[!\"#$%&\\'()*+-/:;<=>?@[\\]^_`{|}~]', re.UNICODE)\n",
    "    input = puctuation_pattern.sub(' ', input)\n",
    "    input = input.replace('  ',' ')\n",
    "    input = input.replace(\"\\n\",' ')\n",
    "    return input\n",
    "df_trimmed['StoryTitleDescription'] = df_trimmed['StoryTitleDescription'].apply(lambda x : remove_punctuation(x))\n",
    "df_trimmed['Acc'] = df_trimmed['Acc'].apply(lambda x : remove_punctuation(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f92be337",
   "metadata": {},
   "outputs": [],
   "source": [
    "stories = df_trimmed.StoryTitleDescription.tolist()\n",
    "acc_criteria = df_trimmed.Acc.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9f8651b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2120.0\n",
      "10600\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "8480"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(len(stories)*0.2)\n",
    "print(len(acc_criteria))\n",
    "10600-2120"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7fe625f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = tf.data.Dataset.from_tensor_slices((stories[:8480], acc_criteria[:8480]))\n",
    "val_dataset = tf.data.Dataset.from_tensor_slices((stories[8480:], acc_criteria[8480:]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3cb281db",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_text(sentence):\n",
    "    sentence = tf.strings.lower(sentence)\n",
    "    # Adding a space between the punctuation and the last word to allow better tokenization\n",
    "    sentence = tf.strings.regex_replace(sentence, r\"([?.!,])\", r\" \\1 \")\n",
    "    # Replacing multiple continuous spaces with a single space\n",
    "    sentence = tf.strings.regex_replace(sentence, r\"\\s\\s+\", \" \")\n",
    "    # Replacing non english words with spaces\n",
    "    sentence = tf.strings.regex_replace(sentence, r\"[^a-z?.!,]+\", \" \")\n",
    "    sentence = tf.strings.strip(sentence)\n",
    "    sentence = tf.strings.join([\"[start]\", sentence, \"[end]\"], separator=\" \")\n",
    "    return sentence\n",
    "\n",
    "\n",
    "vectorizer = layers.TextVectorization(\n",
    "    VOCAB_SIZE,\n",
    "    standardize=preprocess_text,\n",
    "    output_mode=\"int\",\n",
    "    output_sequence_length=MAX_LENGTH,\n",
    ")\n",
    "\n",
    "# We will adapt the vectorizer to both the questions and answers\n",
    "# This dataset is batched to parallelize and speed up the process\n",
    "vectorizer.adapt(tf.data.Dataset.from_tensor_slices((stories + acc_criteria)).batch(128))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f7e42fff",
   "metadata": {},
   "outputs": [],
   "source": [
    "def vectorize_text(inputs, outputs):\n",
    "    inputs, outputs = vectorizer(inputs), vectorizer(outputs)\n",
    "    # One extra padding token to the right to match the output shape\n",
    "    outputs = tf.pad(outputs, [[0, 1]])\n",
    "    return (\n",
    "        {\"encoder_inputs\": inputs, \"decoder_inputs\": outputs[:-1]},\n",
    "        {\"outputs\": outputs[1:]},\n",
    "    )\n",
    "\n",
    "\n",
    "train_dataset = train_dataset.map(vectorize_text, num_parallel_calls=tf.data.AUTOTUNE)\n",
    "val_dataset = val_dataset.map(vectorize_text, num_parallel_calls=tf.data.AUTOTUNE)\n",
    "\n",
    "train_dataset = (\n",
    "    train_dataset.cache()\n",
    "    .shuffle(BUFFER_SIZE)\n",
    "    .batch(BATCH_SIZE)\n",
    "    .prefetch(tf.data.AUTOTUNE)\n",
    ")\n",
    "val_dataset = val_dataset.cache().batch(BATCH_SIZE).prefetch(tf.data.AUTOTUNE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "15c37525",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FNetEncoder(layers.Layer):\n",
    "    def __init__(self, embed_dim, dense_dim, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.embed_dim = embed_dim\n",
    "        self.dense_dim = dense_dim\n",
    "        self.dense_proj = keras.Sequential(\n",
    "            [\n",
    "                layers.Dense(dense_dim, activation=\"relu\"),\n",
    "                layers.Dense(embed_dim),\n",
    "            ]\n",
    "        )\n",
    "        self.layernorm_1 = layers.LayerNormalization()\n",
    "        self.layernorm_2 = layers.LayerNormalization()\n",
    "\n",
    "    def call(self, inputs):\n",
    "        # Casting the inputs to complex64\n",
    "        inp_complex = tf.cast(inputs, tf.complex64)\n",
    "        # Projecting the inputs to the frequency domain using FFT2D and\n",
    "        # extracting the real part of the output\n",
    "        fft = tf.math.real(tf.signal.fft2d(inp_complex))\n",
    "        proj_input = self.layernorm_1(inputs + fft)\n",
    "        proj_output = self.dense_proj(proj_input)\n",
    "        return self.layernorm_2(proj_input + proj_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a4c49121",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEmbedding(layers.Layer):\n",
    "    def __init__(self, sequence_length, vocab_size, embed_dim, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.token_embeddings = layers.Embedding(\n",
    "            input_dim=vocab_size, output_dim=embed_dim\n",
    "        )\n",
    "        self.position_embeddings = layers.Embedding(\n",
    "            input_dim=sequence_length, output_dim=embed_dim\n",
    "        )\n",
    "        self.sequence_length = sequence_length\n",
    "        self.vocab_size = vocab_size\n",
    "        self.embed_dim = embed_dim\n",
    "\n",
    "    def call(self, inputs):\n",
    "        length = tf.shape(inputs)[-1]\n",
    "        positions = tf.range(start=0, limit=length, delta=1)\n",
    "        embedded_tokens = self.token_embeddings(inputs)\n",
    "        embedded_positions = self.position_embeddings(positions)\n",
    "        return embedded_tokens + embedded_positions\n",
    "\n",
    "    def compute_mask(self, inputs, mask=None):\n",
    "        return tf.math.not_equal(inputs, 0)\n",
    "\n",
    "\n",
    "class FNetDecoder(layers.Layer):\n",
    "    def __init__(self, embed_dim, latent_dim, num_heads, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.embed_dim = embed_dim\n",
    "        self.latent_dim = latent_dim\n",
    "        self.num_heads = num_heads\n",
    "        self.attention_1 = layers.MultiHeadAttention(\n",
    "            num_heads=num_heads, key_dim=embed_dim\n",
    "        )\n",
    "        self.attention_2 = layers.MultiHeadAttention(\n",
    "            num_heads=num_heads, key_dim=embed_dim\n",
    "        )\n",
    "        self.dense_proj = keras.Sequential(\n",
    "            [\n",
    "                layers.Dense(latent_dim, activation=\"relu\"),\n",
    "                layers.Dense(embed_dim),\n",
    "            ]\n",
    "        )\n",
    "        self.layernorm_1 = layers.LayerNormalization()\n",
    "        self.layernorm_2 = layers.LayerNormalization()\n",
    "        self.layernorm_3 = layers.LayerNormalization()\n",
    "        self.supports_masking = True\n",
    "\n",
    "    def call(self, inputs, encoder_outputs, mask=None):\n",
    "        causal_mask = self.get_causal_attention_mask(inputs)\n",
    "        if mask is not None:\n",
    "            padding_mask = tf.cast(mask[:, tf.newaxis, :], dtype=\"int32\")\n",
    "            padding_mask = tf.minimum(padding_mask, causal_mask)\n",
    "\n",
    "        attention_output_1 = self.attention_1(\n",
    "            query=inputs, value=inputs, key=inputs, attention_mask=causal_mask\n",
    "        )\n",
    "        out_1 = self.layernorm_1(inputs + attention_output_1)\n",
    "\n",
    "        attention_output_2 = self.attention_2(\n",
    "            query=out_1,\n",
    "            value=encoder_outputs,\n",
    "            key=encoder_outputs,\n",
    "            attention_mask=padding_mask,\n",
    "        )\n",
    "        out_2 = self.layernorm_2(out_1 + attention_output_2)\n",
    "\n",
    "        proj_output = self.dense_proj(out_2)\n",
    "        return self.layernorm_3(out_2 + proj_output)\n",
    "\n",
    "    def get_causal_attention_mask(self, inputs):\n",
    "        input_shape = tf.shape(inputs)\n",
    "        batch_size, sequence_length = input_shape[0], input_shape[1]\n",
    "        i = tf.range(sequence_length)[:, tf.newaxis]\n",
    "        j = tf.range(sequence_length)\n",
    "        mask = tf.cast(i >= j, dtype=\"int32\")\n",
    "        mask = tf.reshape(mask, (1, input_shape[1], input_shape[1]))\n",
    "        mult = tf.concat(\n",
    "            [tf.expand_dims(batch_size, -1), tf.constant([1, 1], dtype=tf.int32)],\n",
    "            axis=0,\n",
    "        )\n",
    "        return tf.tile(mask, mult)\n",
    "\n",
    "\n",
    "def create_model():\n",
    "    encoder_inputs = keras.Input(shape=(None,), dtype=\"int32\", name=\"encoder_inputs\")\n",
    "    x = PositionalEmbedding(MAX_LENGTH, VOCAB_SIZE, EMBED_DIM)(encoder_inputs)\n",
    "    encoder_outputs = FNetEncoder(EMBED_DIM, LATENT_DIM)(x)\n",
    "    encoder = keras.Model(encoder_inputs, encoder_outputs)\n",
    "    decoder_inputs = keras.Input(shape=(None,), dtype=\"int32\", name=\"decoder_inputs\")\n",
    "    encoded_seq_inputs = keras.Input(\n",
    "        shape=(None, EMBED_DIM), name=\"decoder_state_inputs\"\n",
    "    )\n",
    "    x = PositionalEmbedding(MAX_LENGTH, VOCAB_SIZE, EMBED_DIM)(decoder_inputs)\n",
    "    x = FNetDecoder(EMBED_DIM, LATENT_DIM, NUM_HEADS)(x, encoded_seq_inputs)\n",
    "    x = layers.Dropout(0.5)(x)\n",
    "    decoder_outputs = layers.Dense(VOCAB_SIZE, activation=\"softmax\")(x)\n",
    "    decoder = keras.Model(\n",
    "        [decoder_inputs, encoded_seq_inputs], decoder_outputs, name=\"outputs\"\n",
    "    )\n",
    "    decoder_outputs = decoder([decoder_inputs, encoder_outputs])\n",
    "    fnet = keras.Model([encoder_inputs, decoder_inputs], decoder_outputs, name=\"fnet\")\n",
    "    return fnet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "948d2a08",
   "metadata": {},
   "outputs": [],
   "source": [
    "fnet = create_model()\n",
    "fnet.compile(\"adam\", loss=\"sparse_categorical_crossentropy\", metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f2f3e16d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"fnet\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " encoder_inputs (InputLayer)    [(None, None)]       0           []                               \n",
      "                                                                                                  \n",
      " positional_embedding_1 (Positi  (None, None, 256)   2107392     ['encoder_inputs[0][0]']         \n",
      " onalEmbedding)                                                                                   \n",
      "                                                                                                  \n",
      " decoder_inputs (InputLayer)    [(None, None)]       0           []                               \n",
      "                                                                                                  \n",
      " f_net_encoder (FNetEncoder)    (None, None, 256)    263936      ['positional_embedding_1[0][0]'] \n",
      "                                                                                                  \n",
      " outputs (Functional)           (None, None, 8192)   8684288     ['decoder_inputs[0][0]',         \n",
      "                                                                  'f_net_encoder[0][0]']          \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 11,055,616\n",
      "Trainable params: 11,055,616\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "fnet.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "e5c64cfd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/14\n",
      "133/133 [==============================] - 218s 2s/step - loss: 0.9185 - accuracy: 0.8514 - val_loss: 0.8654 - val_accuracy: 0.8871\n",
      "Epoch 2/14\n",
      "133/133 [==============================] - 276s 2s/step - loss: 0.3134 - accuracy: 0.9530 - val_loss: 0.6384 - val_accuracy: 0.9151\n",
      "Epoch 3/14\n",
      "133/133 [==============================] - 265s 2s/step - loss: 0.1428 - accuracy: 0.9772 - val_loss: 0.5954 - val_accuracy: 0.9239\n",
      "Epoch 4/14\n",
      "133/133 [==============================] - 268s 2s/step - loss: 0.0735 - accuracy: 0.9888 - val_loss: 0.5675 - val_accuracy: 0.9323\n",
      "Epoch 5/14\n",
      "133/133 [==============================] - 274s 2s/step - loss: 0.0429 - accuracy: 0.9940 - val_loss: 0.5682 - val_accuracy: 0.9341\n",
      "Epoch 6/14\n",
      "133/133 [==============================] - 319s 2s/step - loss: 0.0303 - accuracy: 0.9957 - val_loss: 0.5662 - val_accuracy: 0.9368\n",
      "Epoch 7/14\n",
      "133/133 [==============================] - 311s 2s/step - loss: 0.0254 - accuracy: 0.9965 - val_loss: 0.5784 - val_accuracy: 0.9378\n",
      "Epoch 8/14\n",
      "133/133 [==============================] - 310s 2s/step - loss: 0.0260 - accuracy: 0.9960 - val_loss: 0.5856 - val_accuracy: 0.9355\n",
      "Epoch 9/14\n",
      "133/133 [==============================] - 291s 2s/step - loss: 0.0478 - accuracy: 0.9915 - val_loss: 0.6127 - val_accuracy: 0.9323\n",
      "Epoch 10/14\n",
      "133/133 [==============================] - 348s 3s/step - loss: 0.0703 - accuracy: 0.9856 - val_loss: 0.6359 - val_accuracy: 0.9294\n",
      "Epoch 11/14\n",
      "133/133 [==============================] - 340s 3s/step - loss: 0.0478 - accuracy: 0.9899 - val_loss: 0.6301 - val_accuracy: 0.9370\n",
      "Epoch 12/14\n",
      "133/133 [==============================] - 285s 2s/step - loss: 0.0216 - accuracy: 0.9960 - val_loss: 0.6315 - val_accuracy: 0.9377\n",
      "Epoch 13/14\n",
      "133/133 [==============================] - 296s 2s/step - loss: 0.0118 - accuracy: 0.9980 - val_loss: 0.6231 - val_accuracy: 0.9400\n",
      "Epoch 14/14\n",
      "133/133 [==============================] - 349s 3s/step - loss: 0.0080 - accuracy: 0.9987 - val_loss: 0.6402 - val_accuracy: 0.9392\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x15a0bf076a0>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fnet.fit(train_dataset, epochs=14, validation_data=val_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "3841197d",
   "metadata": {},
   "outputs": [],
   "source": [
    "VOCAB = vectorizer.get_vocabulary()\n",
    "\n",
    "\n",
    "def decode_sentence(input_sentence):\n",
    "    # Mapping the input sentence to tokens and adding start and end tokens\n",
    "    tokenized_input_sentence = vectorizer(\n",
    "        tf.constant(\"[start] \" + preprocess_text(input_sentence) + \" [end]\")\n",
    "    )\n",
    "    # Initializing the initial sentence consisting of only the start token.\n",
    "    tokenized_target_sentence = tf.expand_dims(VOCAB.index(\"[start]\"), 0)\n",
    "    decoded_sentence = \"\"\n",
    "\n",
    "    for i in range(MAX_LENGTH):\n",
    "        # Get the predictions\n",
    "        predictions = fnet.predict(\n",
    "            {\n",
    "                \"encoder_inputs\": tf.expand_dims(tokenized_input_sentence, 0),\n",
    "                \"decoder_inputs\": tf.expand_dims(\n",
    "                    tf.pad(\n",
    "                        tokenized_target_sentence,\n",
    "                        [[0, MAX_LENGTH - tf.shape(tokenized_target_sentence)[0]]],\n",
    "                    ),\n",
    "                    0,\n",
    "                ),\n",
    "            }\n",
    "        )\n",
    "        # Calculating the token with maximum probability and getting the corresponding word\n",
    "        sampled_token_index = tf.argmax(predictions[0, i, :])\n",
    "        sampled_token = VOCAB[sampled_token_index.numpy()]\n",
    "        # If sampled token is the end token then stop generating and return the sentence\n",
    "        if tf.equal(sampled_token_index, VOCAB.index(\"[end]\")):\n",
    "            break\n",
    "        decoded_sentence += sampled_token + \" \"\n",
    "        tokenized_target_sentence = tf.concat(\n",
    "            [tokenized_target_sentence, [sampled_token_index]], 0\n",
    "        )\n",
    "\n",
    "    return decoded_sentence\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "4763b223",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate acceptance criteria for a new user story\n",
    "t = \"As RSP user , I want to see the dependent stories for current release stories\"\n",
    "d = \"Each story should have the mapping to its dependent stories with dotted representation if it has depedency with the current release story\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "f45cbf51",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 392ms/step\n",
      "1/1 [==============================] - 0s 34ms/step\n",
      "1/1 [==============================] - 0s 36ms/step\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "1/1 [==============================] - 0s 33ms/step\n",
      "1/1 [==============================] - 0s 33ms/step\n",
      "1/1 [==============================] - 0s 33ms/step\n",
      "1/1 [==============================] - 0s 30ms/step\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "1/1 [==============================] - 0s 35ms/step\n",
      "1/1 [==============================] - 0s 30ms/step\n",
      "1/1 [==============================] - 0s 34ms/step\n",
      "1/1 [==============================] - 0s 38ms/step\n",
      "1/1 [==============================] - 0s 38ms/step\n",
      "1/1 [==============================] - 0s 37ms/step\n",
      "1/1 [==============================] - 0s 41ms/step\n",
      "1/1 [==============================] - 0s 34ms/step\n",
      "1/1 [==============================] - 0s 43ms/step\n",
      "1/1 [==============================] - 0s 38ms/step\n",
      "1/1 [==============================] - 0s 33ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 35ms/step\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "1/1 [==============================] - 0s 29ms/step\n",
      "1/1 [==============================] - 0s 30ms/step\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "1/1 [==============================] - 0s 30ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "1/1 [==============================] - 0s 34ms/step\n",
      "1/1 [==============================] - 0s 51ms/step\n",
      "1/1 [==============================] - 0s 37ms/step\n",
      "1/1 [==============================] - 0s 36ms/step\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "1/1 [==============================] - 0s 33ms/step\n",
      "1/1 [==============================] - 0s 33ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'in the summary attribute assigned to ensure when app service user stories for the there are message dc details screen last optimized data to witness relevant use cases and aidt jira associated with caution email notification data will be  '"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decode_sentence(t+d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fa01a1a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
